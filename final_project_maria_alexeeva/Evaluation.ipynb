{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "np.set_printoptions(threshold=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data (the file is called 'dev' because the original provided dataset did not include test data,\n",
    "#only dev data, so dev set was used as test)\n",
    "x_test = np.load('preceding_thread_data/dev_x_preceding_thread.npy')\n",
    "y_test = np.load('preceding_thread_data/dev_y_preceding_thread.npy')\n",
    "id_test = np.load('preceding_thread_data/dev_id_preceding_thread.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with Preceding Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the pretrained model\n",
    "loaded_model = pickle.load(open(\"model_svm_with_preceding_thread.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "predictions = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7986531986531986\n",
      "0.4155921540346482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#calculating and printing the scores\n",
    "print(f1_score(predictions, y_test, average = 'micro'))\n",
    "print(f1_score(predictions, y_test, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0      0.636     0.275     0.384       102\n",
      "        1.0      0.836     0.942     0.886      1181\n",
      "        2.0      0.000     0.000     0.000        82\n",
      "        3.0      0.413     0.375     0.393       120\n",
      "\n",
      "avg / total      0.742     0.799     0.763      1485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#calculating and printing per label scores\n",
    "print(classification_report(y_test, predictions, labels=None, target_names=None, sample_weight=None, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with stand-alone comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pretrained model\n",
    "loaded_model_just_comment_vector = pickle.load(open(\"model_svm_just_comment_vector.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data (the file is called 'dev' because the original provided dataset did not include test data,\n",
    "#only dev data, so dev set was used as test)\n",
    "flatten_dev_x = np.load('flatten_data/flat_dev_x.npy', mmap_mode='r')\n",
    "flatten_dev_y = np.load('flatten_data/flat_dev_y.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the format of the gold labels from categorical to class labels in the test set\n",
    "dev_y_list = []\n",
    "\n",
    "for item in np.asarray(flatten_dev_y):\n",
    "    index = np.where(item == 1)\n",
    "    #print(item, index[0][0])\n",
    "    dev_y_list.append(index[0][0])\n",
    "\n",
    "dev_y = np.asarray(dev_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "predictions_just_comment = loaded_model_just_comment_vector.predict(flatten_dev_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7225589225589225\n",
      "0.4247467981379385\n"
     ]
    }
   ],
   "source": [
    "##calculating and printing the scores\n",
    "print(f1_score(predictions_just_comment, dev_y, average = 'micro'))\n",
    "print(f1_score(predictions_just_comment, dev_y, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.500     0.324     0.393       102\n",
      "          1      0.861     0.798     0.829      1181\n",
      "          2      1.000     0.024     0.048        82\n",
      "          3      0.295     0.792     0.430       120\n",
      "\n",
      "avg / total      0.798     0.723     0.723      1485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculating and printing per label scores\n",
    "print(classification_report(dev_y, predictions_just_comment, labels=None, target_names=None, sample_weight=None, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
